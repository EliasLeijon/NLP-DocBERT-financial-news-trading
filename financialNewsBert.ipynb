{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox7G3ZYq4yjt"
      },
      "source": [
        "## Importing swe-BERT for initial training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s_lPyCgF4yjw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# collab command to install transformers\n",
        "# !pip install transformers\n",
        "# !pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XOPF9_mE4yjx"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "import re\n",
        "def clean_txt (text):\n",
        "  text = re.sub(\"¹\", \"\", text)\n",
        "  text=re.sub(\"(\\\\W)+\",\"  \", text)\n",
        "  return text\n",
        "\n",
        "\n",
        "class SNLIDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, max_size=None):\n",
        "        super().__init__()\n",
        "        self.xs = []\n",
        "        self.ys = []\n",
        "        self.sentence_lengths = np.array([])\n",
        "        count = 0\n",
        "        with open(filename, encoding=\"utf-8\") as source:\n",
        "            for i, line in enumerate(source):\n",
        "                if i == 0:\n",
        "                  continue\n",
        "                # print(line)\n",
        "                if max_size and i >= max_size:\n",
        "                    break\n",
        "                try:\n",
        "                  sentence, sentiment_value = line.rstrip().split('|') # Delimeter to be chosen\n",
        "                  count += 1\n",
        "                except:\n",
        "                  print( \"Error when processing the following data \", [line.rstrip().split('|')])\n",
        "                # print(sentence)\n",
        "                self.xs.append(clean_txt(sentence))\n",
        "                self.ys.append(int(sentiment_value)) # make sure negative/neutral/positive is labelled correct\n",
        "                self.sentence_lengths = np.append(self.sentence_lengths, len(sentence.split(\" \")))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.xs[idx], self.ys[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrCpUve_4yjy"
      },
      "source": [
        "## Create all datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU2qKdwb4yjy",
        "outputId": "e09ed608-1ef2-4fb0-f4db-c1a732e7bb41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finska  flygbolaget  Finnair  inleder  de  tillfälliga  uppsägningarna  av  kabinpersonal  i  februari  2010  \n",
            "COPYRIGHT  AFX  News  och  AFX  Financial  News  Logo  är  registrerade  varumärken  som  tillhör  AFX  News  Limited\n",
            "1232.0\n",
            "78.0\n",
            "2.0\n"
          ]
        }
      ],
      "source": [
        "financial_news_train_dataset = SNLIDataset('./Financial Data/financial_phrases_labeled_psv_train.csv', max_size=4000)\n",
        "financial_news_test_dataset = SNLIDataset('./Financial Data/financial_phrases_labeled_psv_test.csv')\n",
        "amazon_review_dataset = SNLIDataset('./amazon-review-data/amazon_review_data_psv.csv', max_size = 10000)\n",
        "# data = financial_news_train_dataset[121]\n",
        "print(financial_news_train_dataset.xs[0])\n",
        "print(financial_news_test_dataset.xs[0])\n",
        "print(max(amazon_review_dataset.sentence_lengths))\n",
        "print(max(financial_news_train_dataset.sentence_lengths))\n",
        "print(min(financial_news_train_dataset.sentence_lengths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0tgZwEq4yj0"
      },
      "source": [
        "## Import swedish bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ngd8efgP4yj0"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('KB/bert-base-swedish-cased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojp1xQBq4yj0",
        "outputId": "e738417f-48b9-4bca-ed42-4852e56122bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Motsvarande  ökning  av  aktiekapitalet  totalt  300  00  euro  registrerades  i  det  finska  handelsregistret  den  8  maj  2008  \n",
            "tensor([[    2,  3487,  7320,    65, 32700,  3581,  3029,   207,  3605, 39661,\n",
            "            31,    82,  5723, 13788, 23679,    97,   319,  1425,  3003,     3]])\n",
            "{'input_ids': tensor([[    2,  3487,  7320,    65, 32700,  3581,  3029,   207,  3605, 39661,\n",
            "            31,    82,  5723, 13788, 23679,    97,   319,  1425,  3003,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "tokenized = tokenizer(text=financial_news_train_dataset[1][0], padding='longest', return_tensors='pt')\n",
        "print(financial_news_train_dataset[1][0])\n",
        "print(tokenized.input_ids)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyx9uwrO4yj1"
      },
      "source": [
        "### Define colate function that tokenizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "n42CPT4o4yj1"
      },
      "outputs": [],
      "source": [
        "def our_collate_fn(data):\n",
        "    x = [a[0] for a in data]\n",
        "    y = [a[1] for a in data]\n",
        "    tokenized = tokenizer(text=x, padding='longest', return_tensors='pt')\n",
        "\n",
        "    if tokenized['input_ids'].shape[1] > 512:\n",
        "      return tokenized['input_ids'][:,0:511], torch.as_tensor(y)[0:511], tokenized['attention_mask'][:,0:511]\n",
        "    return tokenized['input_ids'], torch.as_tensor(y), tokenized['attention_mask']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3wjfjP4yj2"
      },
      "source": [
        "#### Functions for training and testing a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "eOjHNMEv4yj2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "def train_model(train_data, batch_size):\n",
        "  batch_sort_order = np.array_split(train_data.sentence_lengths.argsort()[::-1], round(len(train_data) / batch_size))\n",
        "  tokenized_train_data = DataLoader(train_data, collate_fn=our_collate_fn, batch_sampler=batch_sort_order) #\n",
        "  \n",
        "  # print(tokenized_train_data)\n",
        "  # for batch in tokenized_train_data:\n",
        "  #     for sent_pair in batch[0]:\n",
        "  #       print(sent_pair)\n",
        "  #     print(batch)\n",
        "  #     break\n",
        "  \n",
        "\n",
        "  model = BertForSequenceClassification.from_pretrained('KB/bert-base-swedish-cased', num_labels=3)\n",
        "  model = model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "  # softmax = torch.nn.Softmax(dim=1)\n",
        "  epochs = 1\n",
        "\n",
        "  for _ in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total=len(train_data)) as pbar:\n",
        "\n",
        "      for bindex, (bx, by, ba) in enumerate(tokenized_train_data):\n",
        "        bx, by, ba = bx.to(device), by.to(device), ba.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        train_output = model(bx, labels=by, attention_mask=ba)\n",
        "        # backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.update(len(bx))\n",
        "  return model\n",
        "\n",
        "def evaluate_model(model, valid_data, batch_size):\n",
        "    batch_sort_order = np.array_split(valid_data.sentence_lengths.argsort()[::-1], round(len(valid_data) / batch_size))\n",
        "    tokenized_valid_data = DataLoader(valid_data, batch_sampler=batch_sort_order, collate_fn=our_collate_fn)\n",
        "    model.eval()\n",
        "    valids = []\n",
        "    for bx, by, ba in tokenized_valid_data:\n",
        "      with torch.no_grad():\n",
        "        bx, by, ba = bx.to(device), by.to(device), ba.to(device)\n",
        "        # forward pass\n",
        "        try:\n",
        "          eval_output = model(bx, attention_mask=ba)\n",
        "          guess = torch.argmax(eval_output.logits, dim=1)\n",
        "          valids.append(sum(guess == by)/len(by))\n",
        "        except Exception as e:\n",
        "          print(bx.shape, by.shape)\n",
        "          print(ba.shape)\n",
        "          print(e)\n",
        "        \n",
        "    print('Accuracy: {}'.format(sum(valids)/len(valids)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and save the financial model"
      ],
      "metadata": {
        "id": "6lnkrQf9sEhx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "9QzggwTk4yj2",
        "outputId": "4678a497-4599-4605-a8f6-42d23af7f492"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-d61c55308954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinancial_trained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinancial_news_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinancial_trained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinancial_news_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfinancial_trained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./financial_trained_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-a9493a9cd95e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_data, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KB/bert-base-swedish-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m             \u001b[0;31m# set dtype to instantiate the model under:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         typed_storage._storage._set_from_file(\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             torch._utils._element_size(typed_storage.dtype))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "financial_trained_model = train_model(financial_news_train_dataset, 64)\n",
        "evaluate_model(financial_trained_model, financial_news_test_dataset)\n",
        "financial_trained_model.save_pretrained(\"./financial_trained_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the financial model and evaluate it on similar test-data \n",
        "accuracy should be 88%+"
      ],
      "metadata": {
        "id": "rSzk4FKwsMni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loadedModel = BertForSequenceClassification.from_pretrained(\"./financial_trained_model/\")\n",
        "evaluate_model(loadedModel, financial_news_test_dataset, 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGjjZWoQdFxa",
        "outputId": "1ffe247c-a3f2-456f-a76c-34d1f99e6c60"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8862680196762085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(loadedModel, amazon_review_dataset, 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPXYKeFSqEhS",
        "outputId": "06c83920-dddf-46cb-b3d2-e9d469ab3e01"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.39497217535972595\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "43b1b630598a744212fd16e5095ab3aa6d4637b887bcabe04ae192328f97bb8f"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}