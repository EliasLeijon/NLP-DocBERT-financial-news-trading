{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox7G3ZYq4yjt"
      },
      "source": [
        "## Importing swe-BERT for initial training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_lPyCgF4yjw",
        "outputId": "1face555-0484-47cb-daf7-5651e2502922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "# collab command to install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XOPF9_mE4yjx"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "import re\n",
        "def clean_txt (text):\n",
        "  text = re.sub(\"¹\", \"\", text)\n",
        "  text=re.sub(\"(\\\\W)+\",\"  \", text)\n",
        "  return text\n",
        "\n",
        "\n",
        "class SNLIDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, max_size=None):\n",
        "        super().__init__()\n",
        "        self.xs = []\n",
        "        self.ys = []\n",
        "        self.sentence_lengths = np.array([])\n",
        "        count = 0\n",
        "        with open(filename, encoding=\"utf-8\") as source:\n",
        "            for i, line in enumerate(source):\n",
        "                if i == 0:\n",
        "                  continue\n",
        "                # print(line)\n",
        "                if max_size and i >= max_size:\n",
        "                    break\n",
        "                try:\n",
        "                  sentence, sentiment_value = line.rstrip().split('|') # Delimeter to be chosen\n",
        "                  count += 1\n",
        "                except:\n",
        "                  print( \"Error when processing the following data \", [line.rstrip().split('|')])\n",
        "                # print(sentence)\n",
        "                self.xs.append(clean_txt(sentence))\n",
        "                self.ys.append(int(sentiment_value)) # make sure negative/neutral/positive is labelled correct\n",
        "                self.sentence_lengths = np.append(self.sentence_lengths, len(sentence.split(\" \")))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.xs[idx], self.ys[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)\n",
        "\n",
        "class SNLIDataset_shell_class(SNLIDataset):\n",
        "    def __init__(self, xs, ys, sentence_lengths):\n",
        "      self.xs = xs\n",
        "      self.ys = ys\n",
        "      self.sentence_lengths = sentence_lengths"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def divide_dataset_to_train_and_test(SNLIDataset, percentage_to_train):\n",
        "    random_indices = torch.randperm(len(SNLIDataset.xs))\n",
        "    num_of_sent_in_train = math.floor(len(SNLIDataset.xs)*(percentage_to_train))\n",
        " \n",
        "    train_dataset_xs = np.array(list(map(SNLIDataset.xs.__getitem__, random_indices[0:num_of_sent_in_train])))\n",
        "    train_dataset_ys = np.array(list(map(SNLIDataset.ys.__getitem__, random_indices[0:num_of_sent_in_train])))\n",
        "    train_dataset_sent_lengths = np.array(list(map(SNLIDataset.sentence_lengths.__getitem__, random_indices[0:num_of_sent_in_train])))\n",
        "\n",
        "\n",
        "    test_dataset_xs = np.array(list(map(SNLIDataset.xs.__getitem__, random_indices[num_of_sent_in_train:])))\n",
        "    test_dataset_ys = np.array(list(map(SNLIDataset.ys.__getitem__, random_indices[num_of_sent_in_train:])))\n",
        "    test_dataset_sent_lengths = np.array(list(map(SNLIDataset.sentence_lengths.__getitem__, random_indices[num_of_sent_in_train:])))\n",
        "  \n",
        "    train_dataset = SNLIDataset_shell_class(train_dataset_xs, train_dataset_ys, train_dataset_sent_lengths)\n",
        "\n",
        "    test_dataset = SNLIDataset_shell_class(test_dataset_xs, test_dataset_ys, test_dataset_sent_lengths)\n",
        "    \n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "WggYx43umASg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrCpUve_4yjy"
      },
      "source": [
        "## Create all datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NU2qKdwb4yjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a92937-85b3-4edd-9c64-92572b5e9da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n",
            "27000\n",
            "3000\n"
          ]
        }
      ],
      "source": [
        "financial_news_train_dataset = SNLIDataset('./Financial Data/financial_phrases_labeled_psv_train.csv')\n",
        "financial_news_test_dataset = SNLIDataset('./Financial Data/financial_phrases_labeled_psv_test.csv')\n",
        "amazon_review_dataset = SNLIDataset('./amazon-review-data/amazon_review_data_psv.csv')\n",
        "# hp_n, mp_n, neg_n, neu_n = SNLIDataset('./sweOnlyProcData/highPosNews.txt'), SNLIDataset('./sweOnlyProcData/mediumPosNews.txt'), SNLIDataset('./sweOnlyProcData/negativeNews.txt'), SNLIDataset('./sweOnlyProcData/neutralNews.txt')\n",
        "\n",
        "# data = financial_news_train_dataset[121]\n",
        "\n",
        "amazon_review_train_dataset, amazon_review_test_dataset = divide_dataset_to_train_and_test(amazon_review_dataset, 0.9)\n",
        "\n",
        "\n",
        "print(len(amazon_review_dataset))\n",
        "print(len(amazon_review_train_dataset))\n",
        "print(len(amazon_review_test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(financial_news_train_dataset.xs[:10]) # Dataseten är på något vis olika formatterad som gör att det blir fel senare när man tränar modellen\n",
        "print(amazon_review_train_dataset.xs[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjvEvCNo0Vs8",
        "outputId": "ccaecceb-3eba-406d-9152-8145f0b2c9d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Finska  flygbolaget  Finnair  inleder  de  tillfälliga  uppsägningarna  av  kabinpersonal  i  februari  2010  ', 'Motsvarande  ökning  av  aktiekapitalet  totalt  300  00  euro  registrerades  i  det  finska  handelsregistret  den  8  maj  2008  ', 'Under  det  tredje  kvartalet  av  räkenskapsåret  2008  svängde  Efore  till  en  nettoförlust  på  400  000  EUR  jämfört  med  en  nettovinst  på  200  000  EUR  för  motsvarande  period  2007  ', 'ALEXANDRIA  Virginia  15  oktober  Aaron  Moss  från  Hampshire  Storbritannien  har  utvecklat  en  dekorativ  design  för  en  telefon  meddelade  US  Patent  amp  Trademark  Office  ', 'Vaisala  Oyj  Börsmeddelande  26  03  2010  klo  09  00  1  1  Årsstämman  i  Vaisala  Oyj  beslutade  den  25  mars  2010  att  godkänna  bolagets  årsredovisning  för  2009  ', 'Produktens  framkantsvikning  maximerar  vädertålighet  och  möjliggör  en  sömlös  takfinish  ', 'Raute  är  noterat  på  den  nordiska  börsen  i  Helsingfors  ', 'KESKO  FOOD  AB  PRESSMEDDELANDE  04  01  2006  KL  13  00  Kesko  Livs  Ab  inleder  en  stor  TV  kampanj  för  att  rekrytera  så  många  som  hundra  nya  K  mathandlare  ', 'W  Ærtsil  Æs  mål  är  att  betjäna  det  snabbt  växande  antalet  fartyg  och  ökande  marina  aktivitet  i  Barentshavsregionen  ', 'En  liten  url  länk  tar  användare  till  en  bedrägeriwebbplats  som  lovar  att  användare  kan  tjäna  tusentals  dollar  genom  att  bli  en  Google  NASDAQ  GOOG  Cash  annonsör  ']\n",
            "['Korgarna  är  inte  som  visas  de  är  mörkt  persikorosa'\n",
            " 'älskade  det  här  och  älskade  att  vi  hade  ett  spel  för  de  quot  små  quot  tjejerna  att  spela  på  vår  babyshower'\n",
            " 'Verkannons  beskrivs  Jag  gillar  att  den  svänger'\n",
            " 'Fruktansvärd  Den  fäster  inte  bra  den  skalade  på  några  minuter  '\n",
            " 'Jag  har  precis  fått  min  och  jag  älskar  den  Den  är  lite  hård  men  mjuknar  upp  när  du  tuggar  på  den  Jag  gillar  inte  sladden  den  kommer  på  så  jag  lägger  en  längre  egentillverkad  sladd  på  den  Men  allt  som  allt  är  det  väldigt  lugnande  och  avkopplande  '\n",
            " 'Det  är  andra  gången  jag  beställer  dessa  haklappar  Jag  broderar  och  lägger  till  värmeöverföringsvinyl  till  dem  utan  problem  '\n",
            " 'Den  här  väskan  håller  BRA  när  du  checkar  in  vagnen  vid  grinden  när  du  reser  Det  skyddar  en  redan  dyr  vagn  från  stötar  och  risker  att  förvaras  i  bagaget  den  sitter  tätt  och  tar  att  vänja  sig  vid  när  du  försöker  montera  in  vagnen  vid  grinden  Jag  rekommenderar  att  du  tränar  hemma  så  att  du  inte  stressar  på  terminalen  '\n",
            " 'Min  dotter  gillar  detta'\n",
            " 'Perfekt  för  tvättbara  våtservetter  Inte  för  att  bada  '\n",
            " 'Bara  okej']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCW4rqz4uF31"
      },
      "source": [
        "## Dataset analysis\n",
        "Here we analyze the length distribution for each dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "hCIcaM4quF31",
        "outputId": "438b93bf-6f0e-4ae6-d942-f7f2f1066ff4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4dba08fdad1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplot_data_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_plots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplot_data_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_review_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"amazon_review_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplot_data_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinancial_news_test_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinancial_news_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"financial_news_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplot_data_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhp_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmp_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Own-collected-news\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'financial_news_test_dataset' is not defined"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_length(sentence_lengths, datasetName, resolution=1):\n",
        "    plot_data_length.total_plots += 1\n",
        "    plt.figure(plot_data_length.total_plots)\n",
        "    plt.hist(sentence_lengths, bins=int(np.max(sentence_lengths)*resolution), density=True)\n",
        "    plt.xlabel(datasetName)\n",
        "plot_data_length.total_plots = 0\n",
        "plot_data_length(amazon_review_dataset.sentence_lengths, \"amazon_review_dataset\", 1/50)\n",
        "plot_data_length(np.concatenate([financial_news_test_dataset.sentence_lengths, financial_news_train_dataset.sentence_lengths]), \"financial_news_dataset\", 1/4)\n",
        "plot_data_length(np.concatenate([hp_n.sentence_lengths, mp_n.sentence_lengths, neg_n.sentence_lengths, neu_n.sentence_lengths]), \"Own-collected-news\", 1/500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0tgZwEq4yj0"
      },
      "source": [
        "## Import swedish bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngd8efgP4yj0",
        "outputId": "7bca235b-50c5-423d-cec8-ee5a7547f315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('KB/bert-base-swedish-cased', do_lower_case=True)\n",
        "bert = BertModel.from_pretrained('KB/bert-base-swedish-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojp1xQBq4yj0",
        "outputId": "1101d06d-cad0-4e58-825d-40556b3a4581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "älskade  det  här  och  älskade  att  vi  hade  ett  spel  för  de  quot  små  quot  tjejerna  att  spela  på  vår  babyshower\n",
            "tensor([[    2,  9360,    82,   382,    36,  9360,    48,   186,   365,   137,\n",
            "           442,    43,   102, 17510,   129,  1459, 17510,   129, 17203,    48,\n",
            "          2055,    68,   671, 18511, 17168,     6,     3]])\n",
            "{'input_ids': tensor([[    2,  9360,    82,   382,    36,  9360,    48,   186,   365,   137,\n",
            "           442,    43,   102, 17510,   129,  1459, 17510,   129, 17203,    48,\n",
            "          2055,    68,   671, 18511, 17168,     6,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "tokenized = tokenizer(text=amazon_review_train_dataset[1][0], padding='longest', return_tensors='pt')\n",
        "print(amazon_review_train_dataset[1][0])\n",
        "print(tokenized.input_ids)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyx9uwrO4yj1"
      },
      "source": [
        "### Define colate function that tokenizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SB98CebIuF33"
      },
      "outputs": [],
      "source": [
        "def get_split(text1):\n",
        "  l_total = []\n",
        "  l_parcial = []\n",
        "  if text1.shape[0]//150 >0:\n",
        "    n = text1.shape[0]//150\n",
        "  else: \n",
        "    n = 1\n",
        "  for w in range(n):\n",
        "    if w == 0:\n",
        "      l_parcial = text1[:200]\n",
        "      l_total.append(l_parcial)\n",
        "    else:\n",
        "      l_parcial = text1[w*150:w*150 + 200]\n",
        "      l_total.append(l_parcial)\n",
        "  return l_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ASAcPEuauF33"
      },
      "outputs": [],
      "source": [
        "def tensor_split(text1, seq_size=200, overlap=50, add_to_start = 2):\n",
        "  l_total = []\n",
        "  l_parcial = []\n",
        "  cls_tokens = torch.unsqueeze(torch.as_tensor([add_to_start]* text1.shape[0]), dim=1)\n",
        "  if text1.shape[1]//(seq_size-overlap) >0:\n",
        "    n = text1.shape[1]//(seq_size-overlap)\n",
        "  else: \n",
        "    n = 1\n",
        "  for w in range(n):\n",
        "    if w == 0:\n",
        "      l_parcial = torch.concat([cls_tokens, text1[:,:seq_size]], dim=1)\n",
        "      l_total.append(l_parcial.to(device))\n",
        "    else:\n",
        "      l_parcial = torch.concat([cls_tokens, text1[:,w*(seq_size-overlap):w*(seq_size-overlap) + seq_size]], dim=1)\n",
        "      l_total.append(l_parcial.to(device))\n",
        "  return l_total\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n42CPT4o4yj1"
      },
      "outputs": [],
      "source": [
        "def our_collate_fn(data):\n",
        "    x = [a[0] for a in data]\n",
        "    y = [a[1] for a in data]\n",
        "    tokenized = tokenizer(text=x, padding='longest', return_tensors='pt')\n",
        "\n",
        "    return tokenized['input_ids'], torch.as_tensor(y), tokenized['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ULmii5oWuF33"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_training_examples(dataset, batch_size = 64, seq_size = 200, overlap = 50):\n",
        "    batch_sort_order = np.array_split(dataset.sentence_lengths.argsort()[::-1], round(len(dataset) / batch_size))\n",
        "    tokenized_train_data = DataLoader(dataset, collate_fn=our_collate_fn, batch_sampler=batch_sort_order) #\n",
        "\n",
        "    for bindex, (bx, by, ba) in enumerate(tokenized_train_data):\n",
        "        yield tensor_split(bx, seq_size, overlap), tensor_split(ba, seq_size, overlap, add_to_start=1), by.to(device)\n",
        "                \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXhye472uF34",
        "outputId": "7b284fff-cee4-42a1-ceb5-02836410f57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: [3, 5, 10]\n",
            "Output shape: [3, 5, 20]\n",
            "Last hidden state shape: [1, 3, 20]\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "rnn = nn.LSTM(10, 20, batch_first = True)\n",
        "input = torch.randn(3, 5, 10)\n",
        "h0 = torch.randn(1, 3, 20)\n",
        "c0 = torch.randn(1, 3, 20)\n",
        "output, (hn, cn) = rnn(input, (h0, c0))\n",
        "\n",
        "print(\"Input shape: {}\".format([*input.shape]))\n",
        "print(\"Output shape: {}\".format([*output.shape]))\n",
        "print(\"Last hidden state shape: {}\".format([*hn.shape]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Y35jjoeIuF34"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DocBert(nn.Module):\n",
        "    def __init__(self, bert, hidden_dim=20, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.bert =  bert.to(device) # INTE BertForSequenceClassification\n",
        "        self.lstm = nn.LSTM(bert.config.hidden_size, hidden_dim, batch_first=True).to(device)\n",
        "        # Input [Batch_size, sequence_length, input_size]\n",
        "        # Output [1, batch_size, hidden_dim]\n",
        "        self.linear = nn.Linear(hidden_dim, num_labels).to(device)\n",
        "    \n",
        "    def forward(self, x_seqs, a_seqs):\n",
        "      output = []\n",
        "      for x_seq, a_seq in zip(x_seqs, a_seqs):\n",
        "        output.append(self.bert(x_seq, a_seq).pooler_output) # Only get the embedding of the [CLS]-token [batch_size, number_of_sequences, input_size]\n",
        "      _ , (output, _) = self.lstm(torch.stack(output, dim=1)) # [1, batch_size, hidden_dim]\n",
        "      return self.linear(torch.squeeze(output, dim=0))\n",
        "      #   bertified_seqs.append()\n",
        "      # self.lstm(, )\n",
        "\n",
        "    def predict(self, x_seq, a_seq): # No attention mask here, correct?\n",
        "      guesses = []\n",
        "      output = self.forward(x_seq, a_seq)\n",
        "      batch_guess = torch.argmax(output, dim=1)\n",
        "      return batch_guess\n",
        "\n",
        "    def accuracy(self, sequences, batch_size=8):\n",
        "      current_correct = 0\n",
        "      current_tried = 0\n",
        "      for x_seq, a_seq, by in create_training_examples(sequences, batch_size=batch_size):\n",
        "        prediction = self.predict(x_seq, a_seq)\n",
        "        current_correct += torch.sum(prediction==by)\n",
        "        current_tried += batch_size\n",
        "      return current_correct / current_tried\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3wjfjP4yj2"
      },
      "source": [
        "#### Functions for training and testing a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "PzLbWwOxR3Qf"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def train_docbert(pretrained_bert, dataset, hidden_dim=20, num_labels=3, epochs=1):\n",
        "  docbert = DocBert(pretrained_bert, hidden_dim, num_labels)\n",
        "  optimizer = torch.optim.Adam(docbert.parameters())\n",
        "  for epoch in range(epochs):\n",
        "    i = 0\n",
        "    tot_loss = 0\n",
        "    for x_seqs, a_seqs, by in create_training_examples(dataset, batch_size=8, seq_size=200, overlap=50):\n",
        "      optimizer.zero_grad()\n",
        "      preds = docbert.forward(x_seqs, a_seqs)\n",
        "      loss = F.cross_entropy(preds, by)\n",
        "      tot_loss += loss\n",
        "      i += 1\n",
        "      if i % 10 == 0:\n",
        "        print(tot_loss / 10)\n",
        "        tot_loss = 0\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    return docbert\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docbert = DocBert(bert)"
      ],
      "metadata": {
        "id": "X2XFxwK04K5t"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = docbert.accuracy(amazon_review_test_dataset)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TdkWgvdUkrZ",
        "outputId": "acadfdcc-a125-480b-a548-36ebd2e86e65"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4157, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOjHNMEv4yj2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "def train_model(train_data, batch_size):\n",
        "  batch_sort_order = np.array_split(train_data.sentence_lengths.argsort()[::-1], round(len(train_data) / batch_size))\n",
        "  tokenized_train_data = DataLoader(train_data, collate_fn=our_collate_fn, batch_sampler=batch_sort_order) #\n",
        "  \n",
        "  # print(tokenized_train_data)\n",
        "  # for batch in tokenized_train_data:\n",
        "  #     for sent_pair in batch[0]:\n",
        "  #       print(sent_pair)\n",
        "  #     print(batch)\n",
        "  #     break\n",
        "  \n",
        "\n",
        "  model = BertForSequenceClassification.from_pretrained('KB/bert-base-swedish-cased', num_labels=3)\n",
        "  model = model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "  # softmax = torch.nn.Softmax(dim=1)\n",
        "  epochs = 1\n",
        "\n",
        "  for _ in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total=len(train_data)) as pbar:\n",
        "\n",
        "      for bindex, (bx, by, ba) in enumerate(tokenized_train_data):\n",
        "        bx, by, ba = bx.to(device), by.to(device), ba.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        train_output = model(bx, labels=by, attention_mask=ba)\n",
        "        # backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.update(len(bx))\n",
        "  return model\n",
        "\n",
        "def evaluate_model(model, valid_data, batch_size):\n",
        "    batch_sort_order = np.array_split(valid_data.sentence_lengths.argsort()[::-1], round(len(valid_data) / batch_size))\n",
        "    tokenized_valid_data = DataLoader(valid_data, batch_sampler=batch_sort_order, collate_fn=our_collate_fn)\n",
        "    model.eval()\n",
        "    valids = []\n",
        "    for bx, by, ba in tokenized_valid_data:\n",
        "      with torch.no_grad():\n",
        "        bx, by, ba = bx.to(device), by.to(device), ba.to(device)\n",
        "        # forward pass\n",
        "        try:\n",
        "          eval_output = model(bx, attention_mask=ba)\n",
        "          guess = torch.argmax(eval_output.logits, dim=1)\n",
        "          valids.append(sum(guess == by)/len(by))\n",
        "        except Exception as e:\n",
        "          print(bx.shape, by.shape)\n",
        "          print(ba.shape)\n",
        "          print(e)\n",
        "        \n",
        "    print('Accuracy: {}'.format(sum(valids)/len(valids)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lnkrQf9sEhx"
      },
      "source": [
        "## Train and save the financial model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_trained_model = train_model(amazon_review_dataset, 64)\n",
        "evaluate_model(amazon_trained_model, amazon_review_dataset)\n",
        "amazon_trained_model.save_pretrained(\"./amazon_trained_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "VH9MwnkTwkN9",
        "outputId": "9ca045bc-f3aa-4cb2-bf46-4c3bb1929e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/30000 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-916a686c92b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mamazon_trained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_review_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_trained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamazon_review_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mamazon_trained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./amazon_trained_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-a9493a9cd95e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_data, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtrain_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1561\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1564\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mbuffered_token_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m                 \u001b[0mbuffered_token_type_ids_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffered_token_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffered_token_type_ids_expanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1526) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [64, 1526].  Tensor sizes: [1, 512]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QzggwTk4yj2",
        "outputId": "e2b4ee32-2cae-4418-de79-7922cf0a8fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 4551/4551 [00:35<00:00, 128.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.78515625\n"
          ]
        }
      ],
      "source": [
        "financial_trained_model = train_model(financial_news_train_dataset, 64)\n",
        "evaluate_model(financial_trained_model, financial_news_test_dataset, 64)\n",
        "# financial_trained_model.save_pretrained(\"./financial_trained_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSzk4FKwsMni"
      },
      "source": [
        "## Load the financial model and evaluate it on similar test-data \n",
        "accuracy should be 88%+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGjjZWoQdFxa",
        "outputId": "f75b65c5-655e-44ab-e8e1-982c1c9bf71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7852193117141724\n"
          ]
        }
      ],
      "source": [
        "# loadedModel = BertForSequenceClassification.from_pretrained(\"./financial_trained_model/\")\n",
        "evaluate_model(financial_trained_model, financial_news_test_dataset, 32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "yPXYKeFSqEhS",
        "outputId": "e85901d7-7c80-4e64-86dc-5cdabbe6c245"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-138be4aef80d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamazon_review_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "evaluate_model(loadedModel, amazon_review_dataset, 16)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vCW4rqz4uF31"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "43b1b630598a744212fd16e5095ab3aa6d4637b887bcabe04ae192328f97bb8f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}